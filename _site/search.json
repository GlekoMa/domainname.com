[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stulink",
    "section": "",
    "text": "Jun 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "others/tips/tip1.html",
    "href": "others/tips/tip1.html",
    "title": "Tip 1",
    "section": "",
    "text": "```{r}\nlibrary(showtext)\nshowtext_auto()\n# 之后再作图\n```"
  },
  {
    "objectID": "others/regression/xt6.6.html",
    "href": "others/regression/xt6.6.html",
    "title": "习题6.6",
    "section": "",
    "text": "判断多重共线性\n\nlibrary(haven)\ndata = read_sav(\"data/xt5.9.sav\")\nhead(data)\n\n# A tibble: 6 × 8\n   年份    x1    x2    x3     x4    x5    x6     y\n  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n1  1978 1018. 1607   138.  96259 2239. 50760 1132.\n2  1979 1259. 1770.  144.  97542 2619. 39370 1146.\n3  1980 1359. 1996.  196.  98705 2976. 44530 1160.\n4  1981 1546. 2048.  207. 100072 3309. 39790 1176.\n5  1982 1762. 2162.  221. 101654 3638. 33130 1212.\n6  1983 1961. 2376.  271. 103008 4020. 34710 1367.\n\n\n\n# 建立模型\nmodel = lm(y ~., data[, -1])\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = data[, -1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-374.18  -82.44   -3.00   91.05  237.52 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.348e+03  2.211e+03   0.610 0.551859    \nx1          -6.410e-01  1.669e-01  -3.840 0.001804 ** \nx2          -3.170e-01  2.044e-01  -1.551 0.143216    \nx3          -4.127e-01  5.485e-01  -0.752 0.464294    \nx4          -2.110e-03  2.428e-02  -0.087 0.931962    \nx5           6.711e-01  1.280e-01   5.241 0.000125 ***\nx6          -7.541e-03  8.128e-03  -0.928 0.369220    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 191.8 on 14 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9946 \nF-statistic:   618 on 6 and 14 DF,  p-value: 3.81e-16\n\n\n方差扩大因子法\n\nlibrary(car)\nvif(model)\n\n         x1          x2          x3          x4          x5          x6 \n 319.484477 2636.564359  479.287849   27.177337 1860.726476    1.742651 \n\n\n特征根判定法\n\nCor = cor(data[, 2:7])\nkappa(Cor, exact=TRUE)\n\n[1] 21642.62\n\n\n答：方差扩大因子\\(VIF_2=2636\\)远大于10，条件数k=21642远大于1000，说明自变量间存在严重的多重共线性。\n\n\n消除多重共线性\n剔除方差扩大因子最大的\\(x_2\\)，重新建立回归方程\n\nmodel2 = lm(y ~ x1 + x3 + x4 + x5 + x6, data)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ x1 + x3 + x4 + x5 + x6, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-376.09 -103.03  -36.15  113.64  301.70 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.253e+03  1.508e+03  -0.831 0.419165    \nx1          -7.354e-01  1.626e-01  -4.524 0.000403 ***\nx3          -9.233e-01  4.588e-01  -2.012 0.062495 .  \nx4           2.639e-02  1.659e-02   1.591 0.132532    \nx5           5.098e-01  7.809e-02   6.528 9.56e-06 ***\nx6          -1.052e-02  8.259e-03  -1.274 0.221998    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 200.6 on 15 degrees of freedom\nMultiple R-squared:  0.9956,    Adjusted R-squared:  0.9941 \nF-statistic: 677.6 on 5 and 15 DF,  p-value: < 2.2e-16\n\nvif(model2)\n\n        x1         x3         x4         x5         x6 \n276.968819 306.617361  11.605489 632.895698   1.645146 \n\n\n\\(VIF_5=632.896>10\\)，再剔除\\(x_5\\)建立回归方程\n\nmodel3 = lm(y ~ x1 + x3 + x4 + x6, data)\nsummary(model3)\n\n\nCall:\nlm(formula = y ~ x1 + x3 + x4 + x6, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-611.00 -112.86  -14.74  172.65  888.36 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -2.715e+03  2.829e+03  -0.960   0.3516  \nx1          -4.738e-02  2.348e-01  -0.202   0.8426  \nx3           1.463e+00  5.260e-01   2.781   0.0133 *\nx4           3.637e-02  3.135e-02   1.160   0.2630  \nx6           3.125e-03  1.516e-02   0.206   0.8393  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 380.7 on 16 degrees of freedom\nMultiple R-squared:  0.9831,    Adjusted R-squared:  0.9788 \nF-statistic: 232.3 on 4 and 16 DF,  p-value: 5.979e-14\n\nvif(model3)\n\n        x1         x3         x4         x6 \n160.512580 111.949275  11.507017   1.539699 \n\n\n\\(VIF_1=160.513>10\\)，再剔除\\(x_1\\)建立回归方程\n\nmodel4 = lm(y ~ x3 + x4 + x6, data)\nsummary(model4)\n\n\nCall:\nlm(formula = y ~ x3 + x4 + x6, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-628.52 -109.40   -0.69  165.52  913.37 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.296e+03  1.870e+03  -1.228    0.236    \nx3           1.359e+00  9.681e-02  14.036 8.84e-11 ***\nx4           3.143e-02  1.906e-02   1.649    0.117    \nx6           3.702e-03  1.446e-02   0.256    0.801    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 369.8 on 17 degrees of freedom\nMultiple R-squared:  0.983, Adjusted R-squared:   0.98 \nF-statistic: 328.2 on 3 and 17 DF,  p-value: 3.055e-15\n\nvif(model4)\n\n      x3       x4       x6 \n4.018087 4.508706 1.484981 \n\n\n答：直至现在，所有自变量的方差扩大因子都已小于10，多重共线性已消除。但变量\\(x_6\\)的t检验P值=0.901>0.05未通过检验，遂将其剔除，再次建立回归模型。\n\nmodel5 = lm(y ~ x3 + x4, data)\nsummary(model5)\n\n\nCall:\nlm(formula = y ~ x3 + x4, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-643.14 -105.66   -4.29  168.60  908.91 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.307e+03  1.820e+03  -1.267   0.2212    \nx3           1.359e+00  9.426e-02  14.415  2.5e-11 ***\nx4           3.304e-02  1.752e-02   1.886   0.0755 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 360.1 on 18 degrees of freedom\nMultiple R-squared:  0.983, Adjusted R-squared:  0.9811 \nF-statistic: 519.3 on 2 and 18 DF,  p-value: < 2.2e-16\n\n\n答：新的回归模型中，变量\\(x_4\\)的t检验P值=0.076，显著性较弱，但选择保留。回归方程为：\\(\\hat{y} = -2.307\\times 10^3+1.359x_3+3.304\\times 10^{-2}x_4\\)。可以看出，在本题中，根据方差扩大因子法所剔除的变量正是后退法和逐步回归法所保留的变量。"
  },
  {
    "objectID": "others/regression/the_data.html",
    "href": "others/regression/the_data.html",
    "title": "习题数据",
    "section": "",
    "text": "数据文件\n\n\n\n\n\n\n Download xt2.14.sav\n\n\n\n\n\n\n\n\n\n Download xt3.11.sav\n\n\n\n\n\n\n\n\n\n Download xt4.9.sav\n\n\n\n\n\n\n\n\n\n Download xt5.9.sav\n\n\n\n\n\n\n\n\n\n Download xt7.6.sav\n\n\n\n\n\n\n\n\n\n Download xt9.2.sav\n\n\n\n\n\n\n\n\n\n Download xt10.3.sav"
  },
  {
    "objectID": "others/regression/xt4.9.html",
    "href": "others/regression/xt4.9.html",
    "title": "习题4.9",
    "section": "",
    "text": "library(haven)\ndata = read_sav(\"data/xt4.9.sav\")\nhead(data)\n\n# A tibble: 6 × 3\n   序号     x     y\n  <dbl> <dbl> <dbl>\n1     1   679  0.79\n2     2   292  0.44\n3     3  1012  0.56\n4     4   493  0.79\n5     5   582  2.7 \n6     6  1156  3.64\n\n\n\n1. 用普通最小二乘法建立\\(y\\)与\\(x\\)的回归方程，并画出残差散点图\n\nmodel0 = lm(y ~ x, data)\nmodel0\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n  -0.831304     0.003683  \n\n\n答：回归方程为：\\(\\hat{y} = -0.8313 + 0.0037x\\)。\n\nlibrary(ggplot2)\nggplot(model0, aes(x=x, y=.resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) + \n    labs(y=\"Residuals\")\n\n\n\n\n\n\n2. 诊断该问题是否存在异方差性\n\n# 等级相关系数法（Spearman检验）\ncor.test(data$x, \n         abs(resid(model0)), \n         alternative='two.sided',\n         method=\"spearman\", \n         conf.level=0.95)\n\n\n    Spearman's rank correlation rho\n\ndata:  data$x and abs(resid(model0))\nS = 16928, p-value = 0.02091\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3175294 \n\n\n答：\\(|e_i|\\)与\\(x_i\\)的等级相关系数=0.318，P值=0.021，存在异方差性。\n\n\n3. 如果存在异方差性，用幂指数型的权函数建立加权最小二乘回归方程\n\n# 编写寻找最优权函数\n# 特定于幂指型权函数的一元回归\nbest_weight_1d = function(data, left=-2, right=2){\n    the_seq = seq(left, right, 0.5)\n    vec = rep(NA, length(the_seq))\n    j = 1\n    for (m in the_seq){\n        model1 = lm(y ~ x,\n                    weights=1/x^m,\n                    data=data)\n        vec[j] = logLik(model1) # logLik函数计算模型的对数似然值\n        j = j + 1\n    }\n    best_local = which.max(vec)\n    if(best_local != left & best_local != right){\n        cat(\"幂指数最优取值为:\", the_seq[best_local])\n    }else if(best_local == left){\n        cat(\"最优值在边界达到，请扩充范围（left）\")\n    }else{\n        cat(\"最优值在边界达到，请扩充范围（right）\")\n    }\n}\nbest_weight_1d(data)\n\n幂指数最优取值为: 1.5\n\n\n\n# 令m=1.5\nmodel1 = lm(y ~ x, weights=1/x^1.5, data=data)\nmodel1\n\n\nCall:\nlm(formula = y ~ x, data = data, weights = 1/x^1.5)\n\nCoefficients:\n(Intercept)            x  \n  -0.683463     0.003557  \n\n\n答：加权最小二乘回归方程为：\\(\\hat{y} = -0.683 + 0.00356x\\)。等级相关系数为-0.076，P值为0.591，异方差性已消除。因\\(R^2=0.659\\)，小于普通最小二乘的0.705，说明加权最小二乘的效果不好。\n\n\n4. 用方差稳定变换\\(y'=\\sqrt{y}\\)消除异方差性\n\ndata$y = sqrt(data$y)\nmodel2 = lm(y ~ x, data)\nmodel2\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n  0.5822259    0.0009529  \n\n\n答：经过方差稳定性变换后的回归方程为：\\(\\hat{y}^{'} = 0.5822 + 0.0009529x\\)。等级相关系数为0.160，P值为0.254，异方差性已消除。因\\(R^2=0.710\\)，优于普通最小二乘的效果。"
  },
  {
    "objectID": "others/regression/xt7.6.html",
    "href": "others/regression/xt7.6.html",
    "title": "习题7.6",
    "section": "",
    "text": "library(haven)\ndata = read_sav(\"data/xt7.6.sav\", encoding=\"gbk\")\nhead(data)\n\n# A tibble: 6 × 6\n  分行编号     y    x1    x2    x3    x4\n     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1        1   0.9  67.3   6.8     5  51.9\n2        2   1.1 111.   19.8    16  90.9\n3        3   4.8 173     7.7    17  73.7\n4        4   3.2  80.8   7.2    10  14.5\n5        5   7.8 200.   16.5    19  63.2\n6        6   2.7  16.2   2.2     1   2.2\n\n\n\n1. 计算\\(y\\)与其余4个变量的简单相关系数\n\ndata = data[, -1] # 删去分行编号\ncor(data)\n\n           y        x1        x2        x3        x4\ny  1.0000000 0.8435714 0.7315050 0.7002815 0.5185181\nx1 0.8435714 1.0000000 0.6787718 0.8484164 0.7797022\nx2 0.7315050 0.6787718 1.0000000 0.5858315 0.4724310\nx3 0.7002815 0.8484164 0.5858315 1.0000000 0.7466458\nx4 0.5185181 0.7797022 0.4724310 0.7466458 1.0000000\n\n\n答：可以看出，\\(y\\)与自变量\\(x_1,x_2,x_3,x_4\\)间的线性相关性较强，但同时自变量间的线性相关性也普遍较强（相关系数接近1）。\n\n\n2. 建立不良贷款\\(y\\)对4个自变量的线性回归方程，所得的回归系数是否合理？\n\nmodel = lm(y ~., data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9198 -0.9507 -0.2880  1.0334  3.1037 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.02164    0.78237  -1.306  0.20643   \nx1           0.04004    0.01043   3.837  0.00103 **\nx2           0.14803    0.07879   1.879  0.07494 . \nx3           0.01453    0.08303   0.175  0.86285   \nx4          -0.02919    0.01507  -1.937  0.06703 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.779 on 20 degrees of freedom\nMultiple R-squared:  0.7976,    Adjusted R-squared:  0.7571 \nF-statistic:  19.7 on 4 and 20 DF,  p-value: 1.035e-06\n\n\n答：在所建立的回归模型中，自变量\\(x_2,x_3,x_4\\)未通过t检验（P值>0.05），回归系数\\(\\beta_2,\\beta_3,\\beta_4\\)不显著。由实际问题来看，\\(x_4\\)的系数显然不能为负，故所得的回归系数不合理。\n\n\n3. 分析回归模型的共线性\n方差扩大因子法\n\nlibrary(car)\nvif(model)\n\n      x1       x2       x3       x4 \n5.330807 1.889860 3.834823 2.781220 \n\n\n特征根判定法\n\nCor = cor(data[, 2:5])\nkappa(Cor, exact=TRUE)\n\n[1] 23.23595\n\n\n答：由方差扩大因子法，未见多重共线性；由特征根判定法，条件数k=23.24>10，模型存在较强多重共线性。\n\n\n4. 采用后退法和逐步回归法选择变量，所得回归方程的回归系数是否合理，是否还存在共线性？\n\n# 后退法\nmodel_back = step(model, direction=\"backward\", trace=0)\nmodel_back\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = data)\n\nCoefficients:\n(Intercept)           x1           x2           x4  \n   -0.97160      0.04104      0.14886     -0.02850  \n\n\n\n# 逐步回归法\nmodel_both = step(model, direction=\"both\", trace=0)\nmodel_both\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = data)\n\nCoefficients:\n(Intercept)           x1           x2           x4  \n   -0.97160      0.04104      0.14886     -0.02850  \n\n\n后退法和逐步回归法筛选出的变量相同\n\nsummary(model_both)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8531 -0.8766 -0.3685  0.9586  3.0772 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.971605   0.711240  -1.366   0.1864    \nx1           0.041039   0.008525   4.814 9.31e-05 ***\nx2           0.148858   0.076817   1.938   0.0662 .  \nx4          -0.028502   0.014206  -2.006   0.0579 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.737 on 21 degrees of freedom\nMultiple R-squared:  0.7973,    Adjusted R-squared:  0.7683 \nF-statistic: 27.53 on 3 and 21 DF,  p-value: 1.802e-07\n\n\n删去不显著的自变量\\(x_2\\)，重新建立回归模型\n\nmodel2 = lm(y ~ x1 + x4, data)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ x1 + x4, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7178 -1.1585 -0.3882  1.2416  5.0093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.443424   0.696865  -0.636   0.5311    \nx1           0.050332   0.007477   6.732 9.14e-07 ***\nx4          -0.031903   0.014954  -2.133   0.0443 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.843 on 22 degrees of freedom\nMultiple R-squared:  0.761, Adjusted R-squared:  0.7393 \nF-statistic: 35.03 on 2 and 22 DF,  p-value: 1.45e-07\n\n\n答：回归方程为：\\(\\hat{y} = -0.443+0.05x_1-0.032x_4\\)。\\(x_4\\)系数为负，不合常理，说明存在多重共线性。\n\n\n5. 建立不良贷款\\(y\\)对4个自变量的岭回归\n\nlibrary(MASS)\n# 与课本所言不同，lm.ridge函数会自动对变量进行标准化\n# 但又如课本所言，lm.ridge的岭参数 == k*n\n# 下面计算k属于(0, 0.2)时的岭回归\nn = nrow(data)\nridge = lm.ridge(y ~., \n                 data, \n                 lambda=seq(0, 0.2*n, 0.1)*nrow(data))\n\n\n# 岭迹图\n# 注：x轴刻度除以n为k值\nplot(ridge)\n\n\n\n\n\n# 附：编写函数，输入原始数据，返回ggplot岭迹图\nggplot_ridge = function(data){\n    # 标准化与岭回归\n    library(MASS)\n    data_scaled = as.data.frame(scale(data))\n    ridge = lm.ridge(y ~., data_scaled, lambda=seq(0, 2*nrow(data), 0.2*nrow(data)))\n\n    coef_ridge = as.data.frame(coef(ridge))\n    coef_ridge[, 1] = row.names(coef_ridge)\n    nrow_cr = nrow(coef_ridge)\n    ncol_cr = ncol(coef_ridge)\n\n    # 宽列表转换为长列表\n    library(tidyr)\n    coef_ridge_long = pivot_longer(coef_ridge, \n                                  cols=2:all_of(ncol_cr),\n                                  names_to = \"beta\", \n                                  values_to = \"value\")\n\n    # 岭迹图\n    library(ggplot2)\n    p = ggplot(coef_ridge_long, aes(x=V1, y=value, group=beta, color=beta)) + \n        geom_smooth(se=FALSE)\n\n    # 岭迹图修饰\n    library(latex2exp)\n    df = coef_ridge[nrow_cr, ]\n    pos = as.numeric(df[, -1])\n    p = p + \n          guides(color=\"none\") + \n          labs(x=TeX(r\"($k\\times n$)\"),\n               y=TeX(r\"($\\hat{\\beta}_j(k)$)\")) + \n          theme(axis.title.x=element_text(hjust=1),\n                axis.title.y=element_text(hjust=1, angle=1)) + \n          theme_minimal()\n    for (i in 1:length(pos)){\n      p = p + annotate(geom=\"text\", \n                       x=df[, 1], \n                       y=pos[i], \n                       label=as.character(i),\n                       vjust=-0.2)\n    }\n\n    return(p)\n}\nggplot_ridge(data)\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n答：由岭迹图，随着岭参数k的增大，\\(\\beta_4\\)由负变正。\n\n\n6. 对第（4）步剔除变量后的回归方程再做岭回归\n\ndata0 = data[, c(\"y\", \"x1\", \"x4\")]\nridge = lm.ridge(y ~ ., data0, lambda=seq(0, 2*n, 0.1))\nplot(ridge)\n\n\n\n\n由岭迹图，岭参数为0.4时的回归系数较为稳定，遂此时的未标准化岭回归方程\n\nridge = lm.ridge(y ~ ., data0, lambda=0.4*n)\nridge\n\n                     x1          x4 \n0.357087614 0.025805860 0.004531316 \n\n\n答：回归方程为：\\(\\hat{y}=0.357+0.0258x_1+0.0045x_2\\)。回归系数都能合理解释。\n\n\n7. 某研究人员希望做\\(y\\)对各项贷款余额、本年累计应收贷款、贷款项目个数这3个自变量的回归，你认为这样做是否可行？如果可行应该如何做？\n用\\(y\\)对\\(x_1,x_2,x_3\\)做岭回归\n\ndata1 = data[, c(\"y\", \"x1\", \"x2\", \"x3\")]\nridge = lm.ridge(y ~ ., data1, lambda=seq(0, 2*n, 0.1))\nplot(ridge)\n\n\n\n\n选取岭参数k=0.4\n\nridge = lm.ridge(y ~ ., data1, lambda=0.4*n)\nridge\n\n                     x1          x2          x3 \n-0.81948673  0.01673907  0.15680666  0.06711093 \n\n\n答：回归方程为：\\(\\hat{y}=-0.819+0.0167x_1+0.157x_2+0.0671x_3\\)，回归系数都能合理解释。"
  },
  {
    "objectID": "others/regression/xt5.9.html",
    "href": "others/regression/xt5.9.html",
    "title": "习题5.9",
    "section": "",
    "text": "library(haven)\ndata = read_sav(\"data/xt5.9.sav\")\nhead(data)\n\n# A tibble: 6 × 8\n   年份    x1    x2    x3     x4    x5    x6     y\n  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n1  1978 1018. 1607   138.  96259 2239. 50760 1132.\n2  1979 1259. 1770.  144.  97542 2619. 39370 1146.\n3  1980 1359. 1996.  196.  98705 2976. 44530 1160.\n4  1981 1546. 2048.  207. 100072 3309. 39790 1176.\n5  1982 1762. 2162.  221. 101654 3638. 33130 1212.\n6  1983 1961. 2376.  271. 103008 4020. 34710 1367.\n\n\n\n# 建立模型\nmodel = lm(y ~., data[, -1])\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = data[, -1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-374.18  -82.44   -3.00   91.05  237.52 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.348e+03  2.211e+03   0.610 0.551859    \nx1          -6.410e-01  1.669e-01  -3.840 0.001804 ** \nx2          -3.170e-01  2.044e-01  -1.551 0.143216    \nx3          -4.127e-01  5.485e-01  -0.752 0.464294    \nx4          -2.110e-03  2.428e-02  -0.087 0.931962    \nx5           6.711e-01  1.280e-01   5.241 0.000125 ***\nx6          -7.541e-03  8.128e-03  -0.928 0.369220    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 191.8 on 14 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9946 \nF-statistic:   618 on 6 and 14 DF,  p-value: 3.81e-16\n\n\n后退法\n\nmodel_back = step(model, direction=\"backward\", trace=0)\nsummary(model_back)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x5, data = data[, -1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-372.27 -102.79   -7.78  157.94  313.69 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 874.58627  106.86620   8.184 2.67e-07 ***\nx1           -0.61116    0.12382  -4.936 0.000125 ***\nx2           -0.35304    0.08840  -3.994 0.000940 ***\nx5            0.63669    0.08914   7.142 1.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.1 on 17 degrees of freedom\nMultiple R-squared:  0.9958,    Adjusted R-squared:  0.9951 \nF-statistic:  1356 on 3 and 17 DF,  p-value: < 2.2e-16\n\n\n逐步回归法\n\n# 逐步回归法\nmodel_both = step(model, direction=\"both\", trace=0)\nsummary(model_both)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x5, data = data[, -1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-372.27 -102.79   -7.78  157.94  313.69 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 874.58627  106.86620   8.184 2.67e-07 ***\nx1           -0.61116    0.12382  -4.936 0.000125 ***\nx2           -0.35304    0.08840  -3.994 0.000940 ***\nx5            0.63669    0.08914   7.142 1.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.1 on 17 degrees of freedom\nMultiple R-squared:  0.9958,    Adjusted R-squared:  0.9951 \nF-statistic:  1356 on 3 and 17 DF,  p-value: < 2.2e-16\n\n\n答：后退法与逐步回归法所保留的变量相同。最终都得到回归方程：\\(\\hat{y}=874.6-0.611x_1-0.353x_2+0.637x_5\\)。但是回归系数的解释不合理。"
  },
  {
    "objectID": "others/regression/xt3.11.html",
    "href": "others/regression/xt3.11.html",
    "title": "习题3.11",
    "section": "",
    "text": "# 课本数据有10行，数据集不知何故多出一行\nlibrary(haven)\ndata = read_sav(\"data/xt3.11.sav\")[-11, ]\nhead(data)\n\n# A tibble: 6 × 4\n      Y    X1    X2    X3\n  <dbl> <dbl> <dbl> <dbl>\n1   160    70    35   1  \n2   260    75    40   2.4\n3   210    65    40   2  \n4   265    74    42   3  \n5   240    72    38   1.2\n6   220    68    45   1.5\n\n\n\n1. 计算出\\(y\\)，\\(x_1\\)，\\(x_2\\)，\\(x_3\\)的相关系数矩阵。\n\ncor(data)\n\n           Y        X1        X2        X3\nY  1.0000000 0.5556527 0.7306199 0.7235354\nX1 0.5556527 1.0000000 0.1129513 0.3983870\nX2 0.7306199 0.1129513 1.0000000 0.5474739\nX3 0.7235354 0.3983870 0.5474739 1.0000000\n\n\n\n\n2. 求\\(y\\)关于\\(x_1\\)，\\(x_2\\)，\\(x_3\\)的三元线性方程\n\nmodel0 = lm(Y ~ X1 + X2 + X3, data)\nmodel0\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3, data = data)\n\nCoefficients:\n(Intercept)           X1           X2           X3  \n   -348.280        3.754        7.101       12.447  \n\n\n答：回归方程为：\\(\\hat{y} = -348.28+3.754x_1 + 7.101x_2 + 12.447x_3\\)。\n\n\n3. 对所求得的方程做拟合优度检验\n\nSm0 = summary(model0)\nSm0$r.squared\n\n[1] 0.8055077\n\n\n答：\\(R^2=0.8055\\)，接近1，说明方程拟合程度较好。\n\n\n4. 对回归方程做显著性检验\n\n# $运算符仅能提取F检验值，而未找到提取F检验P值的办法\n# 其位于summary函数输出的最后一行\nSm0\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.198 -17.035   2.627  11.677  33.225 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -348.280    176.459  -1.974   0.0959 .\nX1             3.754      1.933   1.942   0.1002  \nX2             7.101      2.880   2.465   0.0488 *\nX3            12.447     10.569   1.178   0.2835  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.44 on 6 degrees of freedom\nMultiple R-squared:  0.8055,    Adjusted R-squared:  0.7083 \nF-statistic: 8.283 on 3 and 6 DF,  p-value: 0.01487\n\n\n答：F检验的P值=0.01487<0.05，说明回归方程显著。\n\n\n5. 对每一个回归系数做显著性检验\n\nSm0$coefficients\n\n               Estimate Std. Error   t value   Pr(>|t|)\n(Intercept) -348.280169 176.459221 -1.973715 0.09585537\nX1             3.754037   1.933315  1.941761 0.10019691\nX2             7.100712   2.880281  2.465284 0.04876860\nX3            12.447470  10.569330  1.177697 0.28350986\n\n\n答：对各回归系数t检验中，\\(x_1\\)、\\(x_3\\)相应P值<0.5。在显著性水平5%情况下拒绝原假设，说明\\(x_1\\)、\\(x_3\\)对预测值\\(y\\)没有显著影响。\n\n\n6. 如果有的回归系数没通过显著性检验，将其剔除，重新建立回归方程，再做方程的显著性检验和回归系数的显著性检验\n\n# 由上题结果P值大小，剔除X3\nmodel1 = lm(Y ~ X1 + X2, data)\nSm1 = summary(model1)\nSm1\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.012 -10.656   4.358  11.984  28.927 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -459.624    153.058  -3.003  0.01986 * \nX1             4.676      1.816   2.575  0.03676 * \nX2             8.971      2.468   3.634  0.00835 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.08 on 7 degrees of freedom\nMultiple R-squared:  0.7605,    Adjusted R-squared:  0.6921 \nF-statistic: 11.12 on 2 and 7 DF,  p-value: 0.006718\n\n\n答：剔除\\(x_3\\)后，重新建立的回归方程为：\\(\\hat{y}=-459.624+4.676x_1+8.971x_2\\)。各回归系数相应P值皆小于0.05，即都通过显著性检验。\n\n\n7. 求出每一个回归系数的置信水平为95%的置信区间\n\nconfint(model1)\n\n                   2.5 %     97.5 %\n(Intercept) -821.5473012 -97.700006\nX1             0.3813047   8.969956\nX2             3.1339785  14.807944\n\n\n答：\\(\\hat{\\beta}_0\\)、\\(\\hat{\\beta}_1\\)、\\(\\hat{\\beta}_2\\)的95%置信区间分别为：(-821.55, -97.70)、(0.38, 8.97)和(3.13,14.81)。\n\n\n8. 求标准化回归方程\n\nmodel2 = lm(scale(Y) ~ scale(X1) + scale(X2), data)\nmodel2\n\n\nCall:\nlm(formula = scale(Y) ~ scale(X1) + scale(X2), data = data)\n\nCoefficients:\n(Intercept)    scale(X1)    scale(X2)  \n -7.552e-16    4.792e-01    6.765e-01  \n\n\n答：标准化回归方程为：\\(\\hat{y} = -7.552\\times 10^{-16} + 0.4792x_1 + 0.6765x_2\\)。\n\n\n9. 求当\\(x_{01}=75\\)，\\(x_{02}=42\\)，\\(x_{03}=3.1\\)时的\\(\\hat{y}_0\\)，给定置信水平为95%，计算精确置信区间\n\npredict(model1, \n        newdata=data.frame(X1 = 75, X2 = 42),\n        se.fit=TRUE,\n        interval=\"confidence\",\n        level=.95)$fit\n\n      fit      lwr      upr\n1 267.829 239.9677 295.6903\n\n\n答：\\(\\hat{y}_0=267.829\\)，95%置信水平下的精确置信区间为：(239.97,295.69)。\n\n\n10. 结合回归方程对问题做一些基本分析\n答：1> 由最终建立的回归方程：\\(\\hat{y}=-459.624+4.676x_1+8.971x_2\\)，当固定农业总产值，工业总产值每增加1亿元，货运总量大约将增加4.676万吨。相反同理。而居民非商品指出对货运总量没有显著线性影响。2> 由标准化回归方程：\\(\\hat{y}=-7.552\\times 10^{-16} + 0.4792x_1 + 0.6765x_2\\)，可知农业总产值对货运总量影响更大。"
  },
  {
    "objectID": "others/regression/xt2.14.html",
    "href": "others/regression/xt2.14.html",
    "title": "习题2.14",
    "section": "",
    "text": "# 读取数据\nlibrary(haven)\ndata = read_sav(\"data/xt2.14.sav\")\nhead(data)\n\n# A tibble: 5 × 2\n      x     y\n  <dbl> <dbl>\n1     1    10\n2     2    10\n3     3    20\n4     4    20\n5     5    40\n\n\n\n1. 画散点图\n\nlibrary(ggplot2)\nggplot(data=data, mapping=aes(x=x, y=y)) +\n    geom_point()\n\n\n\n\n\n\n2. \\(x\\)与\\(y\\)之间是否大致呈线性关系\n答：是，由散点图即可看出。\n\n\n3. 用最小二乘估计求出回归方程\n\n# 最小二乘法拟合回归模型\nmodel = lm(y ~ x, data=data)\nmodel\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n         -1            7  \n\n\n答：回归方程为：\\(\\hat{y} = -1 + 7x\\)。\n\n\n4. 求回归标准误差\\(\\hat{\\sigma}\\)\n\n# 查看summary结果\n# 其中Residual standard error即为Se（或称SEE）\nSummary = summary(model)\nSummary$sigma\n\n[1] 6.055301\n\n\n答：回归标准误差\\(\\hat{\\sigma}\\)为：6.055301。\n\n# 附：对lm对象使用summary函数将直接输出模型大部分信息\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n         1          2          3          4          5 \n 4.000e+00 -3.000e+00 -3.775e-15 -7.000e+00  6.000e+00 \nattr(,\"format.spss\")\n[1] \"F8.2\"\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   -1.000      6.351  -0.157   0.8849  \nx              7.000      1.915   3.656   0.0354 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.055 on 3 degrees of freedom\nMultiple R-squared:  0.8167,    Adjusted R-squared:  0.7556 \nF-statistic: 13.36 on 1 and 3 DF,  p-value: 0.03535\n\n\n\n\n5. 给出\\(\\hat{\\beta}_0\\)与\\(\\hat{\\beta}_1\\)的置信度为95%的区间估计\n\nconfint(model)\n\n                  2.5 %   97.5 %\n(Intercept) -21.2112485 19.21125\nx             0.9060793 13.09392\n\n\n答：\\(\\hat{\\beta}_0\\)、\\(\\hat{\\beta}_1\\)的95%置信区间分别为：(-21.21, 19.21)和(0.91, 13.09)。\n\n\n6. 计算\\(x\\)与\\(y\\)的决定系数\n\nSummary$r.squared\n\n[1] 0.8166667\n\n\n答：\\(x\\)与\\(y\\)的决定系数为：0.8167。\n\n\n7. 对回归方程做方差分析\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nx          1    490  490.00  13.364 0.03535 *\nResiduals  3    110   36.67                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n答：F检验的P值=0.03535<0.05，说明回归方程显著。\n\n\n8. 做回归方差\\(\\beta_1\\)的显著性检验\n答：由方差分析表，P值0.0354<0.05，显著性水平为5%情况下拒绝原假设\\(H_0:\\beta_1 = 0\\)，即判定\\(\\beta_1\\)显著不为0。\n\n\n9. 做相关系数的显著性检验\n\ncor.test(data$x, data$y, method='pearson')\n\n\n    Pearson's product-moment correlation\n\ndata:  data$x and data$y\nt = 3.6556, df = 3, p-value = 0.03535\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1057216 0.9936915\nsample estimates:\n      cor \n0.9036961 \n\n\n答：P值=0.03535<0.05，说明相关系数显著。\n解释：对于一元线性回归，三种检验结果是完全一致的。\n\n\n10. 对回归方程作残差图并作相关分析\n\n# 残差图\nggplot(model, aes(x=x, y=.resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) + \n    labs(y=\"Residuals\")\n\n\n\n\n\n# 计算内部学生化残差，然后检验异常值\nestd = rstandard(model)\nsum(abs(estd) > 3) # 大于3Sigma的数量，若为0则判定无异常值\n\n[1] 0\n\n\n答：内部学生化残差\\(|SRE_i|\\)皆小于3，判定无异常值。\n\n# 附：当传入一个线性回归模型作为data时，ggplot会默认对其进行加工\n# 藏而未现的真实数据框为fortify(model)\nhead(fortify(model))\n\n   y x .hat   .sigma      .cooksd .fitted        .resid  .stdresid\n1 10 1  0.6 5.916080 8.181818e-01       6  4.000000e+00  1.0444659\n2 10 2  0.3 6.969321 7.513915e-02      13 -3.000000e+00 -0.5921565\n3 20 3  0.2 7.416198 6.071932e-32      20 -3.774758e-15  0.0000000\n4 20 4  0.3 4.472136 4.090909e-01      27 -7.000000e+00 -1.3816986\n5 40 5  0.6 3.162278 1.840909e+00      34  6.000000e+00  1.5666989\n\n\n\n# 附：对lm对象使用plot函数将直接画出四幅图像\n# 详细解释参考：《R语言教程》李东风 回归诊断一节\n# 1. Residuals vs Fitted\n# 2. Normal QQ\n# 3. Scale-Location\n# 4. Residuals vs Leverage\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11. 求当广告费用为4.2万元时，销售收入将达到多少，并给出置信度为95%的置信区间\n\n# interval=\"predict\"计算因变量新值的区间预测\n# interval=\"confidence\"计算因变量新值的平均值的区间预测\npredict(model, \n        newdata=data.frame(x = 4.2),\n        se.fit=TRUE,\n        interval=\"confidence\",\n        level=.95)$fit\n\n   fit      lwr      upr\n1 28.4 17.09746 39.70254\n\n\n答：销售收入将达28.4万元，预测值的95%置信区间为(17.10,39.70)。因为数据样本量只有5个，所以该区间估计的误差较大。"
  },
  {
    "objectID": "posts/Quarto/CUMCM.html",
    "href": "posts/Quarto/CUMCM.html",
    "title": "Quarto Template for CUMCM",
    "section": "",
    "text": "关于Quarto\nQuato是一个开源的科技出版系统，支持Jupyter、RStudio和VS Code等多种代码编辑器。其官方描述如下：\n\n创建代码与文本结合的可动态编译内容。支持Python、R、Julia和Observable。\n使用markdown文本或Jupyter笔记本的形式进行编译。\n渲染高质量论文、报告、幻灯片、网站、博客和书籍，并以HTML、PDF、MS Word、ePub或其他格式输出。\n支持数学公式、引文、交叉引用、图表、标注、高级布局以及其他特性。\n\n\n\n使用须知\n本文主要介绍Quarto在数学建模论文排版上的应用。关于此数学建模论文模板的使用，需要声明以下几点：\n\n该模板谨按照全国大学生数学建模竞赛的标准进行设计，不适用于美赛、深圳杯等其他数学建模赛事。\n本模板仅适合对排版要求不高的朋友采用。模板主要使用LaTeX编写，由于一篇论文的编译流程大致为：ipynb/qmd -> md -> Pandoc -> PDF，层层抽象过多，故自定义排版布局对使用者相关知识的要求较高，且一旦编写格式逾矩，排错难度较大。\n对模板的过度依赖会导致使用者个人的排版能力下降，即使理想的未来情况为内容创作者不再需要为各式样的排版劳心，但就目前来说，社会对于个人的排版能力仍然有一定要求。\n\n\n\n准备工作\n\n从Quato官网下载并安装Quarto\n使用终端（Terminal）执行命令：\n\nquarto tools install tinytex\n\n安装Typora编辑器beta版\n\n\n\n数学公式\nQuarto使用LaTeX格式的数学公式。LaTeX数学公式是计算机中最为广泛使用的公式输入格式，大多数文字或文档编辑软件都对LaTeX数学公式提供支持，例如：Word、Jupyter、Matlab等。下面介绍Quarto中数学公式的输入。\nLaTeX中数学公式的输入分两种模式：行内模式（$ … $）与行间模式\n$$\n...\n$$\n\n前者是在正文的行文中插入数学公式；后者单独成行，且自动居中。\n例如$E=mc^2$显示为\\(E=mc^2\\),而\n$$\nE=mc^2\n$$\n\n则显示为：\n\\[\nE=mc^2\n\\]\n输入带自动编号的公式则需要使用Quarto的交叉引用功能，例如：\n$$\nE=mc^2\n$$ {#eq-einstein}\n\n\n\n# 第二个$$和{}间的空格不能少\n显示为：\n\\[\nE=mc^2\n\\qquad(1)\\]\n交叉引用功能留待后面小节进行介绍。\n\n\n图片与表格\n\n插入图片\n图片插入请使用’!+[图片名]+(路径/链接)’的形式，支持插入的图片格式包括JPG、PNG、PDF等（不支持SVG），例如：\n![MIT LOGO](_mit.png){#fig-mit}\n\n# “{#fig-MIT}”的用处将在后面进行说明\n显示为：\n\n\n\nFigure 1: MIT LOGO\n\n\n特别的，针对PDF输出，可在{}中添加参数控制输出样式，例如：\n![MIT LOGO](_mit.png){fig.pos=\"H\" width=\"50%\" height=\"25%\"}\n代表禁止图片浮动（有时会失效）、图片宽度占PDF页面的50%，高度占页面的25%。\n\n\n插入表格\n例如：\n| 符号  | 说明        | 单位 |\n|-------|-------------|------|\n| $x_i$ | 第$i$次相遇 | 毫厘 |\n| $y_j$ | 第$j$次错过 | 千里 |\n\n: 符号说明 {#tbl-符号说明}\n\n# “: 符号说明” 代表指定表格名称\n# “{#tbl-符号说明}”的用处将在后面进行说明,其与表名间的空格不可省略\n显示为：\n\n\nTable 1: 符号说明\n\n\n符号\n说明\n单位\n\n\n\n\n\\(x_i\\)\n第\\(i\\)次相遇\n毫厘\n\n\n\\(y_j\\)\n第\\(j\\)次错过\n千里\n\n\n\n\n\n\n\n交叉引用\n本节介绍Quarto的交叉引用，我们在先前小节的数学公式、图片和表格中分别标记了{#eq-einstein}、{#fig-mit}和{#tbl-符号说明}，我们可以在文中任何地方引用他们，格式为@...，例如：\n @eq-einstein 、 @fig-mit 和 @tbl-符号说明\n分别显示为： Equation 1 、 Figure 1 和 Table 1\n\n\n建模模板的使用\n\n下载模板文件cumcm.tex：\n\n\n\n\n\n\n\n Download cumcm.tex\n\n\n\n\n下载文档示例与示例图片：\n\n\n\n\n\n\n\n Download Demo\n\n\n\n\n\n\n\n\n\n Download mit.png\n\n\n\n\n将这三个文件放入同一文件夹，使用终端（Terminal）执行命令：\n\nquarto preview 你的.md文档路径\n\n\n资料与工具\n\n相关资料\nQuato官网\n一份其实很短的LaTeX入门文档\n使用knitr包输出各种类型的LaTeX表格\n\n\n工具\n在线LaTeX公式编辑器\n流程图：Draw.io\n思维导图：XMind\n\n\n\n实际示例\n血管机器人的订购与生物学习（2022年五一杯数学建模竞赛A题）\n\n\n\n\n\n\n Download 2022五一杯A题.zip\n\n\n\n\n\nBUG\nupdating tlmgr\n\nupdating existing packages\n\ncompilation failed- error\nLaTeX3 Error: Mismatched LaTeX support files detected.\n(LaTeX3)        Loading 'expl3.sty' aborted!\n(LaTeX3)\n(LaTeX3)        The L3 programming layer in the LaTeX format\n(LaTeX3)        is dated 2022-06-02, but in your TeX tree the files require\n(LaTeX3)        at least 2022-06-16.\n\nFor immediate help type H <return>.\n ...\n\nl.77      \\ExplLoaderFileDate{expl3.sty}}\n                                         %\n终端输入：\nfmtutil-sys --all"
  },
  {
    "objectID": "posts/Hi/hi.html",
    "href": "posts/Hi/hi.html",
    "title": "Hi",
    "section": "",
    "text": "Is anybody in there?"
  },
  {
    "objectID": "posts/alpha/alpha.html",
    "href": "posts/alpha/alpha.html",
    "title": "希腊字母表",
    "section": "",
    "text": "大写\n小写\n国际音标\n英文\n汉字注音\n\n\n\n\n\n1\nΑ\nα\n/’ælfə/\nalpha\n阿尔法\n\n\n2\nΒ\nβ\n/’bi:tə/ /’beɪtə/\nbeta\n贝塔/毕塔\n\n\n3\nΓ\nγ\n/’gæmə/\ngamma\n伽玛/甘玛\n\n\n4\nΔ\nδ\n/’deltə/\ndelta\n德尔塔/岱欧塔\n\n\n5\nΕ\nε\n/’epsɪlɒn/\nepsilon\n艾普西龙\n\n\n6\nΖ\nζ\n/’zi:tə/\nzeta\n泽塔\n\n\n7\nΗ\nη\n/’i:tə/\neta\n伊塔/诶塔\n\n\n8\nΘ\nθ\n/’θi:tə/\ntheta\n西塔\n\n\n9\nΙ\nι\n/aɪ’əʊtə/\niota\n埃欧塔\n\n\n10\nΚ\nκ\n/’kæpə/\nkappa\n堪帕\n\n\n11\n∧\nλ\n/’læmdə/\nlambda\n兰姆达\n\n\n12\nΜ\nμ\n/mju:/\nmu\n谬/穆\n\n\n13\nΝ\nν\n/nju:/\nnu\n拗/奴\n\n\n14\nΞ\nξ\n希腊: /ksi/ 英美: /ˈzaɪ/或/ˈksaɪ/\nxi\n可西/赛\n\n\n15\nΟ\nο\n/əuˈmaikrən/ /ˈɑmɪˌkrɑn/\nomicron\n欧 (阿~) 米可荣\n\n\n16\n∏\nπ\n/paɪ/\npi\n派\n\n\n17\nΡ\nρ\n/rəʊ/\nrho\n柔/若\n\n\n18\n∑\nσ\n/’sɪɡmə/\nsigma\n西格玛\n\n\n19\nΤ\nτ\n/tɔ:/ /taʊ/\ntau\n套/驼\n\n\n20\nΥ\nυ\n/ˈipsilon/ /ˈʌpsɨlɒn/\nupsilon\n宇 (阿~) 普西龙\n\n\n21\nΦ\nφ\n/faɪ/\nphi\n弗爱/弗忆\n\n\n22\nΧ\nχ\n/kaɪ/\nchi\n凯/柯义\n\n\n23\nΨ\nψ\n/psaɪ/\npsi\n赛/普赛/普西\n\n\n24\nΩ\nω\n/’əʊmɪɡə/ /oʊ’meɡə/\nomega\n欧米伽/欧枚嘎"
  }
]
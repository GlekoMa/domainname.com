{
  "hash": "11fa8a0ee9cb6af854cd54dd9d731770",
  "result": {
    "markdown": "---\ntitle: 习题7.6 \nfig-height: 3\nfig-width: 5\n---\n\n> **数据如下表。用主成分回归方法建立模型，并与其他方法的结果进行比较。**\n\n::: {.cell}\n\n```{.r .cell-code}\n# 课本所提供的数据有误，故未采用其sav文件\ndata = read.csv(\"data\\\\li5.5.csv\")\nhead(data)\n```\n\n::: {.cell-output-stdout}\n```\n  x1 x2 x3 x4     y\n1  7 26  6 60  78.5\n2  1 29 15 52  74.3\n3 11 56  8 20 104.3\n4 11 31  8 47  87.6\n5  7 52  6 33  95.9\n6 11 55  9 22 109.2\n```\n:::\n:::\n\n**普通最小二乘法**\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = lm(y ~ ., data)\nsummary(model)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,\tAdjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n```\n:::\n:::\n\n\n**逐步回归法**\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_step = step(model, direction=\"both\", trace=0)\nsummary(model_step)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,\tAdjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n```\n:::\n:::\n\n*删去不显著变量$x_4$，再拟合新回归模型*\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y ~ x1 + x2, data))\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.893 -1.574 -1.302  1.363  4.048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***\nx1           1.46831    0.12130   12.11 2.69e-07 ***\nx2           0.66225    0.04585   14.44 5.03e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.406 on 10 degrees of freedom\nMultiple R-squared:  0.9787,\tAdjusted R-squared:  0.9744 \nF-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09\n```\n:::\n:::\n\n**主成分回归**\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_scaled = data.frame(scale(data))\npr = princomp(~ x1 + x2 + x3 + x4, data_scaled)\nsummary(pr)\n```\n\n::: {.cell-output-stdout}\n```\nImportance of components:\n                         Comp.1    Comp.2     Comp.3       Comp.4\nStandard deviation     1.436568 1.2061634 0.41503232 0.0387148836\nProportion of Variance 0.558926 0.3940165 0.04665154 0.0004059364\nCumulative Proportion  0.558926 0.9529425 0.99959406 1.0000000000\n```\n:::\n:::\n\n*对前两个主成分做普通最小二乘回归*\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_scaled$z1 = pr$scores[, 1]\ndata_scaled$z2 = pr$scores[, 2]\nmodel_pr = lm(y ~ z1 + z2 - 1, data_scaled)\npr$loadings[,1:2]\n```\n\n::: {.cell-output-stdout}\n```\n       Comp.1     Comp.2\nx1  0.4759552  0.5089794\nx2  0.5638702 -0.4139315\nx3 -0.3940665 -0.6049691\nx4 -0.5479312  0.4512351\n```\n:::\n\n```{.r .cell-code}\nsummary(model_pr)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ z1 + z2 - 1, data = data_scaled)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22139 -0.14545 -0.06309  0.07311  0.29415 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \nz1  0.656958   0.037712  17.421 2.34e-09 ***\nz2 -0.008309   0.044915  -0.185    0.857    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1953 on 11 degrees of freedom\nMultiple R-squared:  0.965,\tAdjusted R-squared:  0.9587 \nF-statistic: 151.8 on 2 and 11 DF,  p-value: 9.787e-09\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthe_coef = coef(model_pr) %*% t(pr$loadings[,1:2])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 借用习题7.6中编写的unscale函数\nunscale = function(data, coef){\n    my_sd = \\(x) (mean((x-mean(x))^2))^0.5 # 定义计算总体标准差函数my_sd\n    coef_sd = apply(data[, -1], 2, my_sd)\n    coef_mean = apply(data[, -1], 2, mean)\n    beta_j = coef / coef_sd\n    beta_0 = sum(beta_j * -coef_mean) + apply(data[, 1], 2, mean)\n    names(beta_0) = \"Intercept\"\n    return(c(beta_0, beta_j))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output-stderr}\n```\n\n载入程辑包：'dplyr'\n```\n:::\n\n::: {.cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\ndata = tibble(data[, c(5, 1:4)])\nunscale(data, the_coef) # 结果与课本不同，待解决\n```\n\n::: {.cell-output-stdout}\n```\n  Intercept                                                 \n94.97563245  0.05457790  0.02500789 -0.04125204 -0.02261707 \n```\n:::\n:::\n\n**偏最小二乘**\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pls)\nmodel_pls = plsr(y ~ ., \n                 data=data.frame(scale(data)), \n                 validation='LOO',\n                 jackknife=TRUE, \n                 method='widekernelpls')\nsummary(model_pls, what='all')\n```\n\n::: {.cell-output-stdout}\n```\nData: \tX dimension: 13 4 \n\tY dimension: 13 1\nFit method: widekernelpls\nNumber of components considered: 4\n\nVALIDATION: RMSEP\nCross-validated using 13 leave-one-out segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps\nCV           1.041   0.2644   0.2239   0.1751   0.1937\nadjCV        1.041   0.2561   0.2059   0.1732   0.1910\n\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps\nX    55.89    62.12    99.96   100.00\ny    96.78    98.16    98.21    98.24\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pls = plsr(y ~ ., \n                 ncomp=3,\n                 data=data.frame(scale(data)), \n                 validation='LOO',\n                 jackknife=TRUE)\nunscale(data, coef(model_pls)) # 结果与课本不同，待解决\n```\n\n::: {.cell-output-stdout}\n```\n   Intercept                                                     \n94.736468452  0.090945117  0.018813126 -0.009696809 -0.026126080 \n```\n:::\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}
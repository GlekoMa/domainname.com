---
title: 《R for Everyone》小注 Part.1
author: ''
date: '2022-01-30'
slug: r-for-everyone
categories: []
tags: []
description: R语言：实用数据分析和可视化技术（原书第2版）(https://book.douban.com/subject/34891734/)...
---
|  
[<img src="https://simpleicons.org/icons/r.svg" style="max-width:15%;min-width:40px;float:right;" alt="R repo" />](https://www.r-project.org/)

> Part.1范围为第1-16章，17-26章与理论结合很紧密，留待Part.2探讨，27-30章则放在Part.3。书是1月30日开始读的，2月5日这篇小注完结。  

> 此书结构好，内容相对较新。  

> 英文版2017年出版，中文版2019年12月出版，而市面上流行的中文R书目，很多都是写作于13、14年，而当时的环境与17年（这一年Hadley Wickam的《R for Data Science》出版，意味着Tidyverse已相对成熟）相比有比较大的差别。  

> 推荐：Garrett Grolemund.[Hands-On Programming with R](https://rstudio-education.github.io/hopr/)  其[中文版](https://book.douban.com/subject/26808217/)是我的启蒙书籍，讲解由三个项目驱动，在没有任何基础的情况下，三天就可看完。（这本书几乎没有使用任何R包，在我看来，其缺点也许只有内容相对较少。而Hadley Wickam的《R for Data Science》是其姊妹篇。）

笔记是一种很私人的东西，网络上许多博客的学习笔记通常过于冗长，且内容常仅是对原书的一些提炼，这类总结性的笔记适用于自己复习，而就效率来说，未必适合他者。所以这篇文章里只包含许多注释性质的东西，故只称作“小注”。

“小注”分为*一般注释*和特针对此书的*特殊注释*。

## 一般注释
### P17 “2.2 RStudio”  
在启动或退出时，建议不要恢复或保存.RData文件。   
这点其他R书籍可能没有顾及到，但设置成“Never”确实能方便许多。

### P61 “6.2 读取Excel数据”  
书中只提到读取Excel文件可以使用readxl包；如果想写入Excel文件的话，可以使用openxlsx包。

## 特殊注释
### P14 “2.2 RStudio”  

本节中讲了一些RStudio里的快捷键，补充如下（Windows）：  

- Alt + - ：快速输入“ <- ”
- Ctrl + Shift + M ：快速输入“ %>% ”（或“ |> ”，可以在RStudio中设置）
- Ctrl + Alt + R ：运行当前脚本所有内容
- Shift + Enter ：在控制台中换行
- Ctrl + Shift +N ：新建脚本
- Ctrl + S ：保存脚本
- Ctrl + W ：关闭当前脚本
- Ctrl + Shift + W ：关闭所有脚本

### P40 “4.8 管道”  

本节中有代码如下：  
```{r}
library(magrittr)
x <- 1:10
mean(x)
```

```{r}
x %>% mean
```
这里的x %>% mean也可以写成x %>% mean()。事实上，R 4.1版本后，原生支持管道，但管道符写法与magrittr包不同，写作 |> ，实现原理和用法也略有不同，比如，使用 |> 必须写成x |> mean()而不能写成x |> mean，统计之都有与此相关的[讨论贴](https://d.cosx.org/d/422269-r-410)

### P46 “5.1 数据框”  

本节中有代码如下：  
```
x <- 10:1
y <- -4:5
q <- c("Hockey","Football","Baseball","Curling","Rugby",
       "Lacrosse","Basketball","Tennis","Cricket","Soccer")
theDF <- data.frame(First=x,Second=y,Sport=q)
#省略···
class(theDF[,"Sport"])
```
书中给出的结果为  
```
[1] "factor"
```
但实际运行结果应为
```
[1] "character"
```
这是因为书中使用的R版本为3.4.0，而R自[4.0.0版本](https://stat.ethz.ch/pipermail/r-announce/2020/000653.html)以后，更新为默认使用stringAsFactors = FALSE，即不再默认将数据框（data.frame）中的列向量转换为factor（因子）向量，所以实际运行结果会与书中有所差异。  
R 4.0.0于2020年4月24日发布，也就是说所有早于这个时间出版的书目都会存在相关问题。因此阅读一些相关资料时应当时刻注意到这个差异。

###  P53 “5.2 列表”

本节有如下叙述： 

_“偶尔对列表或向量或数据框增加元素都还好，但是，如果反复这样做计算代价就太高了。所以最好创建指定长度的列表，然后通过合适的索引增加元素。”_

如果没看懂这段描述的话，学完循环相关章节后，阅读下面两段代码：  

事先指定了变量output的长度：
```{r}
#system.time函数可以计算一段代码的运行时间
system.time({
 output <- rep(NA, 10000000)
 for (i in 1:10000000) {
 output[i] <- i + 1
 }
})
```
未事先指定output的长度：
```{r}
system.time({
 output <- NA
 for (i in 1:10000000) {
 output[i] <- i + 1
 }
})
```
可以看出前者的运行速度大约是后者的3.6倍。原因是在后者中，R不得不每次循环都扩充一次output变量的长度。即每次循环，R都需要在计算机内存中复制原长度的output变量，同时腾出一块容量更大的区域，用来存储新长度的output变量，最后把原位置的output变量清除。在后一段代码中，这一系列创建、复制、粘贴、删除的繁琐操作共进行了10000000次！而前者的写法就很好地避免了这个问题！

### P61 “6.2 读取Excel数据”  

本节中有代码如下：  
```
download.file(url='http://www.jaredlander.com/data/ExcelExample.xlsx',
              destfile='data/ExcelExample.xlsx',method='curl')
```
运行应该会报错：
```
Error in download.file(url = "http://www.jaredlander.com/data/ExcelExample.xlsx",  : 
  'curl' call had nonzero exit status
```
报错原因在于保存路径不对，因为本地的R工作目录可能与作者的不同，工作目录可以用getwd()查看，很可能本地工作目录下面没有创建data文件夹，于是报错。手动创建一个data文件夹即可。

但事情没有这么简单，即使不再报错，从网页写到本地的也可能（或者说一定）只是一个1KB的xlsx文件，也就是说无法使用，即这次写入其实是失败了。具体原因我不清楚，经查找[资料](https://stackoverflow.com/questions/59697422/why-method-curl-is-not-working-in-download-file-in-r/59697717#59697717?newreg=636c77acc9b043e5a2b0f3fe04ae9b17),可以改为这样写（即可成功）：
```
download.file(url='http://www.jaredlander.com/data/ExcelExample.xlsx',
              destfile='data/ExcelExample.xlsx',mode='wb')
```

### P68 “6.7 读取网页数据*”
本节有代码如下：
```
library(XML)
theURL <- "http://www.jaredlander.com/2012/02/another-kind-of-super-bowl-pool/"
bowlPool <- readHTMLTable(theURL,which=1,header=FALSE,stringsAsFactors=FALSE)
```
运行应该会报错：
```
Error: failed to load external entity "http://www.jaredlander.com/2012/02/another-kind-of-super-bowl-pool/"
```
原因我尚未探究，web方面的我比较欠缺，暂只留待以后解决。  
不过若换用课本紧接提到的rvest包，则就可以成功提取：
```
library(rvest)
theURL <- "http://www.jaredlander.com/2012/02/another-kind-of-super-bowl-pool/"
bowlPool <- read_html(theURL) %>% 
                 html_element("table") %>% 
                 html_table()
#read_html() 爬取页面源代码
#html_element() 提取网页中指定元素的部分
#html_table() 根据提取的元素生成数据框或列表
```
课本后面提取特定元素使用的函数是html_nodes（现在还可以使用），官网现在已经没有
这个函数的介绍了，其新的替代函数是html_elements。html_element和html_elements有所
不同，加“s”最终返回的会是一个列表。  
_Tip：XML包很经典，Hadley Wickham参考它开发了xml2包（2015年的blog），两个包孰优孰劣，因为我尚未探究，所以不得而知。_

### P82 “7.2 ggplot2”
本节有代码如下：
```
econ2000 <- economics[which(economics$year >= 2000),]
```
去掉which也可以：
```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(lubridate)
economics$year <- year(economics$date)
econ2000 <- economics[which(economics$year >= 2000),]
re_econ2000 <- economics[economics$year >= 2000,]
all(econ2000 == re_econ2000)
```
（或者说使用which可以提高运行效率？暂按。）

### P109 “11.3 plyr包”
本节有代码如下：  
<1>
```{r message=FALSE, warning=FALSE}
library(plyr)
system.time(
    replicate(1000,dlply(baseball,"id",nrow))
)
#我擅自增加了replicate函数
#因为只运行一次的话几乎运行时长很短，几乎看不出差别
```
<2>
```{r message=FALSE, warning=FALSE}
library(plyr)
iBaseball <- idata.frame(baseball)
system.time(replicate(100,dlply(iBaseball,"id",nrow)))
```
课本中两者的运行时间分别为0.31和0.19，而这里的运行结果是88.89和94.59，为何两个结果不一致，不清楚。不过也许不重要，作者后面也提到将数据集转换为idata.frame格式能不能提高效率还得挑数据集。😵     

<3>   
现在测试使用data.table的效率：
```{r message=FALSE, warning=FALSE}
library(data.table)
data(baseball,package = "plyr")
bBaseball = data.table(baseball)
system.time(
  replicate(1000,
            bBaseball[,list(nrow=length(year)),by=id]
            )
)
#这里使用的是length，原因是nrow在这儿用不了。
#事实上如果把上面的两段代码换成length，运行时间几乎与使用nrow相同。
```
仅用时两秒！😮   

<4>    
再测试使用dplyr包的效率：
```{r message=FALSE, warning=FALSE}
library(dplyr)
data(baseball,package = "plyr")
dBaseball = tibble(baseball)
system.time(
  replicate(1000,
            dBaseball %>% 
              group_by(id) %>% 
              summarize(nrow=nrow(year))
            )
)
```
用时十秒。

### P139 “12.11 dplyr使用数据库”
本节有代码如下：
```
download.file("http://www.jaredlander.com/data/diamonds.db",
              destfile="data/diamonds.db",mode = "wb")
diaDBSource <- src_sqlite("data/diamonds.db")
```
提示错误：找不到src_sqlite函数。原因是作者写作时dplyr包的版本为0.6，现在则为1.0.7。而1.0.0版本的相关更新说明如下： 

> src_mysql(), src_postgres(), and src_sqlite() has been deprecated. We’ve recommended against them for some time. Instead please use the approach described at https://dbplyr.tidyverse.org/.

即现在建议使用dbplyr（dplyr的支援包）来操纵数据库。dbplyr官网中的写法已经不再使用src_sqlite函数，而是改用书中提到的另一种应用面更广的方法：使用DBI::dbConnect()函数。

### P172 “16.3 提取文本”
本节有代码如下：
```
library(XML)
load("dadta/presidents.rdata")
theURL <- "http://www.loc.gov//rr/print/list/057_chron.html"
presidents <- readHTMLTable(theURL,which=3,as.data.frame=TRUE,
                           skip.rows=1,header=TRUE,stringsAsFactors=FALSE)
```
运行应该会报错：
```
Error: failed to load HTTP resource
```
进入这个网址，会发现其设置了验证程序以防止爬取数据。  

解决办法：人工复制这个网页的html源代码，保存为本地html文件，然后从本地爬取。  

第二种爬取办法：保存到本地后，使用之前书上介绍过的rvest包。可以直接爬取出正确的表：
```
library(rvest)
#假设文件被保存为bb.html
theURL <- "bb.html"
presidents2 <- (read_html(theURL) %>% 
    html_elements("table") %>% 
    html_table())[[4]]
```

## One more thing
_注：《R for Everyone》中没有给出练习题目，故应自行寻找巩固知识的办法。_

> 子曰：“学而不思则罔，思而不学则殆。”    ——《论语·为政》